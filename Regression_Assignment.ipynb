{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Regression\n"
      ],
      "metadata": {
        "id": "0_PTjpn1wi4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "- Simple Linear Regression is a statistical method used to model the relationship between two variables — one independent variable (X) and one dependent variable (Y) — by fitting a straight line to the observed data.\n",
        "- The purpose of simple linear regression is to model how changes in the independent variable X are associated with changes in the dependent variable Y. It helps us predict the value of Y based on the value of X, assuming there is a linear relationship between them.\n",
        "      The general equation is:\n",
        "                 Y=mX+c\n",
        "      here:\n",
        "\n",
        "        Y is the dependent variable (target)\n",
        "        X is the independent variable (predictor)\n",
        "        m is the slope of the line\n",
        "        c is the intercept on the Y-axis\n",
        "\n",
        "- The regression line is found by minimizing the difference between the actual values and the predicted values of Y. This difference is typically minimized using the least squares method, which calculates the line that has the smallest sum of squared residuals (errors).\n",
        "- In summary, Simple Linear Regression is a way to quantify and model a linear dependency between two variables and is often used in forecasting, trend analysis, and inferential statistics."
      ],
      "metadata": {
        "id": "_t19ZspDwjBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "- For Simple Linear Regression to provide valid and reliable results, certain key assumptions must be met. These assumptions ensure that the linear model accurately captures the relationship between the independent and dependent variables.\n",
        "- Here are the main assumptions:\n",
        "  -  Linearity - There should be a linear relationship between the independent variable (X) and the dependent variable (Y).\n",
        "  This means the change in Y should be proportional to the change in X. If the relationship is not linear, the model’s predictions will be misleading.\n",
        "  - Independence of Errors (Residuals) - The residuals (differences between actual and predicted Y values) should be independent of each other.\n",
        "  This assumption implies that the value of one observation’s error does not influence another. This is especially important when data is collected over time (time series).\n",
        "  - Homoscedasticity (Constant Variance of Errors) - The variance of residuals should be constant across all levels of the independent variable.If the spread of residuals increases or decreases with X, it indicates heteroscedasticity, which can make the model's estimates unreliable.\n",
        "  -  Normality of Errors - The residuals should be normally distributed.This is important when you're conducting hypothesis tests (e.g., t-tests for coefficients), as these tests assume normality for accurate p-values and confidence intervals.\n",
        "  - No Significant Outliers - There should be no influential outliers that distort the regression line significantly.\n",
        "  Outliers can skew the slope, intercept, and overall fit of the model."
      ],
      "metadata": {
        "id": "snqkfnigwjEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c ?\n",
        "- In the linear regression equation:\n",
        "         Y=mX+c\n",
        "\n",
        "         the coefficient m is known as the slope of the regression line.\n",
        "\n",
        "- It represents the rate of change in the dependent variable (Y) with respect to a one-unit increase in the independent variable (X).\n",
        "- In simple terms:\n",
        "  - If m = 3, then for every 1 unit increase in X, Y increases by 3 units.\n",
        "  - If m = -2, then for every 1 unit increase in X, Y decreases by 2 units.\n",
        "\n",
        "- Interpretation of m:\n",
        "  - If m > 0: Y increases as X increases (positive relationship).\n",
        "  - If m < 0: Y decreases as X increases (negative relationship).\n",
        "  - If m = 0: Y stays constant as X changes (horizontal line).\n",
        "\n",
        "- Why is it important >>\n",
        "  - The slope gives direction and strength of the relationship between X and Y:\n",
        "    - A positive m means a positive relationship (as X increases, Y increases)\n",
        "    - A negative m means a negative relationship (as X increases, Y decreases)\n",
        "    - A slope close to 0 means very little or no linear relationship between X and Y\n"
      ],
      "metadata": {
        "id": "aAo_B_CSwjHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c ?\n",
        "- In the linear equation Y = mX + c, the intercept c represents the value of Y when X is 0.\n",
        "- In simple terms:\n",
        "  - It’s the point where the line crosses the Y-axis on a graph.\n",
        "  - It tells you the starting value of Y , before any effect of X  when (x=0).\n",
        "- Example:\n",
        "   - If the equation is:\n",
        "                        Y=3X+5\n",
        "\n",
        "    - The slope (m) is 3 → this tells you how much Y changes with each unit increase in X.\n",
        "    - The intercept (c) is 5 → this tells you that when X=0,Y=5.\n",
        "    - So, the intercept gives the baseline value of the dependent variable Y, without any influence from the independent variable X."
      ],
      "metadata": {
        "id": "QWTID6_rwjJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression ?\n",
        "- In simple linear regression, the slope m shows how much Y changes when X changes. It is calculated using this formula:\n",
        "           m = Σ[(X - X̄) * (Y - Ȳ)] / Σ[(X - X̄)²]\n",
        "\n",
        "           Where:\n",
        "           X̄ is the mean of X values\n",
        "           Ȳ is the mean of Y values\n",
        "           Σ means summation (add up all the values)\n",
        "\n",
        "- Steps To Calculate >>\n",
        "  - Find the mean of X (X̄) and Y (Ȳ)\n",
        "  - For each data point, subtract the mean and multiply the differences: (X - X̄) * (Y - Ȳ)\n",
        "  - Add up all those results → this is the numerator\n",
        "  - Then square the (X - X̄) values and add them → this is the denominator\n",
        "  - Divide the numerator by the denominator → this is the slope m\n",
        "  "
      ],
      "metadata": {
        "id": "alSgsLUDwjM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression ?\n",
        "- The purpose of the Least Squares Method in Simple Linear Regression is to find the best-fitting straight line through the data points by minimizing the sum of the squared differences between the actual values and the predicted values.\n",
        "- These differences are called residuals. The smaller the total squared residuals, the better the line fits the data.\n",
        "- In other words, the Least Squares Method helps to:\n",
        "  - Accurately estimate the slope (m) and intercept (c) of the line.\n",
        "  - Ensure the line has the least possible error between predicted and actual values.\n",
        "  - Provide the most reliable predictions for the dependent variable (Y) based on the independent variable (X).\n",
        "\n",
        "- It is called \"least squares\" because it minimizes the sum of squared errors:\n",
        "          Σ (Y_actual - Y_predicted)²\n",
        "          This is the key idea behind training a linear regression model\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "W1OoEyGNwjPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "- The coefficient of determination (R²) is a statistical measure that tells us how well the regression line fits the data in Simple Linear Regression.\n",
        "- It is interpreted as the proportion of the variance in the dependent variable (Y) that is explained by the independent variable (X).\n",
        "- Key Points:\n",
        "  - R² values range between 0 and 1.\n",
        "  - If R² = 1, the model explains 100% of the variation in Y (perfect fit).\n",
        "  - If R² = 0, the model explains none of the variation in Y (poor fit).\n",
        "  - A higher R² means the model is better at predicting or explaining the data.\n",
        "- Example:\n",
        "If R² = 0.85, it means 85% of the variation in Y is explained by X, and the remaining 15% is due to other factors or random error."
      ],
      "metadata": {
        "id": "eXtl6yhGwjSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?\n",
        "- Multiple Linear Regression is a statistical technique used to model the relationship between one dependent variable (Y) and two or more independent variables (X₁, X₂, X₃, ... Xn).\n",
        "- It extends Simple Linear Regression, which uses only one independent variable.\n",
        "         General Equation:\n",
        "         Y = b₀ + b₁X₁ + b₂X₂ + b₃X₃ + ... + bnXn + ε\n",
        "         Where:\n",
        "         Y = Dependent variable (the one we want to predict)\n",
        "         X₁, X₂, ..., Xn = Independent variables (predictors)\n",
        "         b₀ = Intercept\n",
        "         b₁, b₂, ..., bn = Coefficients (slopes) for each independent variable\n",
        "         ε = Error term\n",
        "\n",
        "- Purpose:\n",
        "   - The goal of Multiple Linear Regression is to:\n",
        "     - Analyze how several factors influence a single outcome\n",
        "     - Make more accurate predictions by using multiple variables\n",
        "\n",
        "- Example:If you're predicting house prices:\n",
        "         Price = b₀ + b₁*(Size) + b₂*(Location) + b₃*(Age of house)\n",
        "         Each variable contributes to the final prediction.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4cn5x_wLwjVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "- The main difference between Simple and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable.\n",
        "\n",
        "- Simple Linear Regression:\n",
        "  - Uses only one independent variable.\n",
        "  - Equation: Y = b₀ + b₁X + ε\n",
        "  - Example: Predicting salary based on years of experience.\n",
        "  - Number of predictors - One\n",
        "  - Model Complexity -\tLow\n",
        "  - Use Case -\tBasic relationships\n",
        "\n",
        "- Multiple Linear Regression:\n",
        "  - Uses two or more independent variables.\n",
        "  - Equation: Y = b₀ + b₁X₁ + b₂X₂ + ... + bnXn + ε\n",
        "  - Example:Predicting house price based on size, location, and number of bedrooms.\n",
        "  - - Number of predictors - Two or more\n",
        "  - Model Complexity -\tHigher\n",
        "  - Use Case -\tComplex, real-world relationships\n"
      ],
      "metadata": {
        "id": "Ztoc9DQJwjYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression ?\n",
        "- To use Multiple Linear Regression properly, several key assumptions must be satisfied. These assumptions ensure that the model gives valid and reliable results.\n",
        "- Key Assumptions:\n",
        "  - Linearity >>\n",
        "    - There should be a linear relationship between the dependent variable and each independent variable.\n",
        "  - Independence of Errors >>\n",
        "    - The residuals (errors) should be independent of each other.\n",
        "    - No autocorrelation (especially important in time series data).\n",
        "  - Homoscedasticity (Constant Variance of Errors) >>\n",
        "    - The residuals should have constant variance at all levels of the independent variables.\n",
        "    - This means the spread of errors should be even across the regression line.\n",
        "  - Normality of Errors >>\n",
        "    - The residuals should be normally distributed.\n",
        "    - This is especially important for hypothesis testing and confidence intervals.\n",
        "  - No Multicollinearity >>\n",
        "    - The independent variables should not be too highly correlated with each other.\n",
        "    - High multicollinearity can make coefficient estimates unstable.\n",
        "    \n",
        "  "
      ],
      "metadata": {
        "id": "FiPhBS5GwjbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is Heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "- Heteroscedasticity occurs when the variance of the residuals (errors) is not constant across all levels of the independent variables in a regression model.\n",
        "- In simple terms, it means the spread of errors changes as the predicted values change — for example, errors may be small for low values of Y but large for high values.\n",
        "- How to Spot Heteroscedasticity - When plotting residuals vs. predicted values, if the points fan out or form a pattern, it indicates heteroscedasticity.\n",
        "- Effects on Regression Results:\n",
        "  - Unreliable Coefficient Estimates - The estimates of the standard errors become incorrect.\n",
        "  - Incorrect p-values and Confidence Intervals - This may lead to wrong conclusions about which variables are significant.\n",
        "  - Reduced Model Efficiency - Although the coefficients (b-values) remain unbiased, they are no longer efficient, meaning you might not be getting the best predictions.\n",
        "\n",
        "- Ideal Scenario: In a well-behaved model, the errors should show homoscedasticity — that is, constant variance across all observations."
      ],
      "metadata": {
        "id": "aCajmwFlwjeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity ?\n",
        "- Multicollinearity occurs when two or more independent variables in a regression model are highly correlated with each other. This makes it difficult to understand the effect of each variable and leads to unstable and unreliable coefficient estimates.\n",
        "- Ways to Improve the Model:\n",
        " - Remove Highly Correlated Predictors >>\n",
        "   - Check correlation matrix or VIF (Variance Inflation Factor).\n",
        "   - Drop or combine variables with high correlation (e.g., above 0.8).\n",
        " - Use Principal Component Analysis (PCA) >>\n",
        "   - PCA transforms correlated features into a smaller set of uncorrelated components, which can be used in the model.\n",
        " - Apply Regularization Techniques >>\n",
        "   - Use models like Ridge Regression or Lasso Regression which can handle multicollinearity by shrinking or eliminating less important features.\n",
        " - Combine Related Variables >>\n",
        "   - If two variables are highly correlated, you can combine them into a single feature (e.g., by averaging).\n",
        " - Collect More Data >>\n",
        "   - Sometimes, multicollinearity issues reduce when more data is added to the model.\n",
        "  "
      ],
      "metadata": {
        "id": "Qn10nk_fwjhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "- Categorical variables contain labels or categories (like \"Male\"/\"Female\" or \"Red\"/\"Green\"/\"Blue\") which need to be converted into numerical form before they can be used in regression models.\n",
        "- Common Techniques:\n",
        " - Label Encoding >>\n",
        "   - Converts categories into numeric labels (0, 1, 2, etc.)\n",
        "   - Example: [\"Red\", \"Green\", \"Blue\"] - [0, 1, 2]\n",
        "   - Use only when the categories have a natural order (e.g., Low < Medium < High)\n",
        " - One-Hot Encoding >>\n",
        "   - Creates a new binary column for each category\n",
        "   - Example:\"Red\" → [1, 0, 0], \"Green\" → [0, 1, 0], \"Blue\" → [0, 0, 1]\n",
        "   - Suitable for nominal data (no natural order)\n",
        " - Ordinal Encoding >>\n",
        "   - Assigns numbers based on rank or order\n",
        "   - Example: [\"Small\", \"Medium\", \"Large\"] → [1, 2, 3]\n",
        "   - Use when there is a clear hierarchy among categories\n",
        " - Binary Encoding / Target Encoding / Frequency Encoding (Advanced) >>\n",
        "   -  These are more complex and used in high-cardinality features (many unique categories)"
      ],
      "metadata": {
        "id": "YVNAyGuowjjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "- Interaction terms in Multiple Linear Regression are used to capture the combined effect of two or more independent variables on the dependent variable, which may not be explained by their individual effects alone.\n",
        "- Why Interaction Terms Are Important:\n",
        "  - Sometimes, the effect of one variable on the outcome depends on the value of another variable.\n",
        "  - Interaction terms help the model understand and represent such combined influences.\n",
        "-  Example: Suppose you're predicting sales based on Advertising and Season.\n",
        "    - Individually:\n",
        "      - Advertising increases sales.\n",
        "      - Season also affects sales.\n",
        "    - But together:\n",
        "      - The effect of Advertising might be stronger in the festive season.\n",
        " - So, the model could include an interaction term:\n",
        "            Sales = b₀ + b₁*(Advertising) + b₂*(Season) + b₃*(Advertising × Season)\n",
        "            Here, b₃ captures the interaction effect.\n"
      ],
      "metadata": {
        "id": "AMM_QYD_wjmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of the intercept differ between Simple and Multiple Linear Regression?\n",
        "- The intercept is the point where the regression line crosses the Y-axis. It represents the expected value of the dependent variable (Y) when all independent variables (X’s) are equal to zero.\n",
        "- However, the context and interpretation of the intercept can differ depending on the number of independent variables:\n",
        "\n",
        "\n",
        "- In Simple Linear Regression >>\n",
        "  - The intercept (b₀) is the value of Y when X = 0.\n",
        "  - There is only one independent variable.\n",
        "  - It is generally easier to interpret because the relationship is straightforward.\n",
        "  - Example:If you're predicting someone's salary based on years of experience:\n",
        "            Salary = b₀ + b₁*(Experience)\n",
        "\n",
        "    - Then, b₀ (the intercept) tells you the expected salary when experience = 0 (i.e., someone with no experience at all).\n",
        "    - In many real-world cases, this has a clear and realistic meaning.\n",
        "\n",
        "\n",
        "- In Multiple Linear Regression >>\n",
        "  - There are two or more independent variables.\n",
        "  - The intercept now represents the value of Y when all independent variables are equal to zero at the same time.\n",
        "  - This can be difficult or even meaningless to interpret if a situation where all predictors are zero doesn't make sense.\n",
        "  - Example:If you're predicting house price:\n",
        "           Price = b₀ + b₁*(Size) + b₂*(Location Score) + b₃*(Age of House)\n",
        "      - Then b₀ is the predicted price when the Size = 0, Location Score = 0, and Age = 0 — which might not be a realistic or valid combination in the real world.\n",
        "\n",
        "- So, while mathematically the intercept always refers to the predicted Y when inputs are zero, its practical interpretation depends on whether you are working with a simple or a more complex multiple regression model.\n",
        "\n"
      ],
      "metadata": {
        "id": "CoIHitdXwjpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions ?\n",
        "- In regression analysis, the slope is one of the most important components of the regression equation. It represents the rate of change in the dependent variable (Y) for a one-unit increase in the independent variable (X), assuming all other variables remain constant.\n",
        "- Significance of the Slope:\n",
        "  - The slope tells us how strongly and in what direction the independent variable influences the dependent variable.\n",
        "  - Direction of Relationship >>\n",
        "    - If the slope is positive, Y increases as X increases (direct relationship).\n",
        "    - If the slope is negative, Y decreases as X increases (inverse relationship).\n",
        "    - Magnitude of Change >>\n",
        "      - The value of the slope tells you how big the change in Y will be for each unit change in X.\n",
        "      - A larger slope means a stronger influence of X on Y.\n",
        "    - Prediction Power >>\n",
        "      - Slopes are the core of prediction in regression.\n",
        "      - When we input a new value of X, the slope is used to calculate the predicted value of Y.\n",
        "    \n",
        "- In Multiple Linear Regression:Each independent variable has its own slope.Each slope tells you the impact of that variable on the prediction while holding all other variables constant.\n",
        "\n"
      ],
      "metadata": {
        "id": "Iz2rGpkLwjsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "- In a regression model, the intercept is a key component that helps define the relationship between the dependent variable and the independent variable(s). It is typically denoted as β0 in the regression equation and represents the expected value of the dependent variable when all independent variables are equal to zero.\n",
        "       \n",
        "- Role and Contextual Significance of the Intercept:\n",
        " - Baseline Value of the Dependent Variable: The intercept indicates the predicted value of the dependent variable when all independent variables are zero. It acts as the starting point of the regression line on the Y-axis.\n",
        " - Reference for Interpretation of Other Coefficients: The intercept sets the baseline from which the effect of each independent variable is measured. In other words, changes in the dependent variable due to changes in independent variables are calculated relative to the intercept.\n",
        " - Contextual Relevance: The interpretability of the intercept depends on whether the value zero is meaningful for the independent variables. For example, if we are predicting car prices based on mileage, the intercept represents the predicted price of a car with 0 mileage. In this context, it may represent the price of a brand-new car. However, in some cases (e.g., zero age for adult income prediction), the intercept may not be practically meaningful but is still mathematically necessary.\n",
        " - In Multiple Linear Regression: When there are multiple independent variables, the intercept represents the expected value of the dependent variable when all independent variables are zero simultaneously. Its interpretation becomes more complex, especially if the scenario of all predictors being zero is unrealistic.\n",
        "\n",
        "- The intercept in a regression model provides an essential baseline that helps understand the relationship between the variables. While its practical interpretation depends on the context of the data, it remains a critical component of the model for making accurate predictions and interpreting the influence of other variables. Understanding the role of the intercept enables better insights into how changes in independent variables affect the outcome of interest.\n",
        "\n"
      ],
      "metadata": {
        "id": "r_ytFZVawjvK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "- The R-squared (R²) value, also known as the coefficient of determination, is a commonly used statistical measure that indicates the proportion of the variance in the dependent variable that is predictable from the independent variables. While R² provides useful information about the explanatory power of a model, relying solely on it for evaluating model performance has several limitations.\n",
        "\n",
        "- Limitations of Using R² Alone:\n",
        " - Does Not Indicate Predictive Accuracy: A high R² value does not necessarily mean that the model has good predictive power on new or unseen data. It only explains the fit of the model on the current dataset, not its generalization ability.\n",
        " - Insensitive to Overfitting: R² tends to increase as more predictors are added to the model, even if they are not statistically significant. This can lead to overfitting, where the model fits the training data very well but performs poorly on test data.\n",
        " - Does Not Reflect Model Bias or Variance: R² does not provide any information about bias, variance, or errors in predictions. A model with high bias or high variance might still have a moderate or high R².\n",
        " - Cannot Compare Different Types of Models: R² is specific to linear regression and cannot be used to compare the performance of non-linear models, classification models, or models with different target transformations.\n",
        " - Misleading for Non-linear Relationships: If the underlying relationship between variables is non-linear and a linear regression model is used, R² may give a low value even if the relationship is strong but not linear.\n",
        " - Ignores the Scale and Units of Errors: R² does not tell us anything about the actual magnitude of prediction errors. Two models with the same R² could have very different Mean Absolute Error (MAE) or Root Mean Squared Error (RMSE) values.\n",
        " - Not Informative for Models Without Intercept: In models without an intercept term, R² can produce misleading results or even negative values, making its interpretation unreliable.\n",
        "\n",
        "- While R² is a useful metric to understand how well a model explains the variability of the dependent variable, it should not be used as the sole measure of model performance. It is important to use additional metrics such as Adjusted R², RMSE, MAE, MAPE, and cross-validation techniques to comprehensively evaluate a model’s accuracy, robustness, and generalization capability. A balanced use of these metrics leads to more reliable model assessment and better decision-making.\n",
        "\n"
      ],
      "metadata": {
        "id": "iYiIV9I6CzbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "- In regression analysis, the standard error of a regression coefficient measures the variability or uncertainty in the estimate of that coefficient. It reflects how much the coefficient would vary if the same model were fitted to different samples drawn from the same population.\n",
        "\n",
        "- Interpretation of a Large Standard Error: A large standard error for a regression coefficient indicates a high level of uncertainty or instability in the estimated value of that coefficient. This has several important implications:\n",
        "\n",
        " - Low Precision in Coefficient Estimate:\n",
        "    - A large standard error means that the estimate of the coefficient is not precise. In other words, if we were to collect new samples and refit the model, the coefficient could vary widely.\n",
        "\n",
        " - Lower Statistical Significance:\n",
        "   - The standard error is used in calculating the t-statistic and p-value for testing the null hypothesis that the coefficient is equal to zero. A large standard error leads to a smaller t-statistic and a higher p-value, which reduces the likelihood of the coefficient being statistically significant.\n",
        "\n",
        " - Possible Multicollinearity:\n",
        "   - In multiple regression models, a large standard error can be a sign of multicollinearity, where two or more independent variables are highly correlated. This makes it difficult for the model to isolate the individual effect of each predictor, resulting in unstable coefficient estimates.\n",
        "\n",
        " - Model May Lack Useful Information:\n",
        "   - A large standard error may suggest that the corresponding variable does not contribute significantly to explaining the variation in the dependent variable. It might be a candidate for removal, especially if it also has a high p-value.\n",
        "\n",
        " - Wider Confidence Interval:\n",
        "   - A large standard error leads to a wider confidence interval for the coefficient, meaning we are less confident about the true value of the parameter.\n",
        "\n",
        "- A large standard error for a regression coefficient indicates greater uncertainty in the estimated effect of that predictor. It may reflect low reliability, insufficient data, multicollinearity, or that the variable is not significantly related to the outcome. In such cases, further analysis is needed to assess the model structure, remove irrelevant variables, or gather more data to improve the robustness of the estimates."
      ],
      "metadata": {
        "id": "Ej7yJ9bpCzy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
        "- In regression analysis, heteroscedasticity refers to a condition where the variance of residuals (errors) is not constant across all levels of the independent variable(s). This violates a key assumption of ordinary least squares (OLS) regression, which assumes homoscedasticity, or constant variance of residuals.\n",
        "\n",
        "- Identifying Heteroscedasticity in Residual Plots:\n",
        " - Heteroscedasticity can be visually detected using residual plots, where residuals are plotted against:\n",
        "   - The predicted (fitted) values, or\n",
        "   - An independent variable.\n",
        "\n",
        " - Common patterns that indicate heteroscedasticity:\n",
        "   - Fan Shape (Cone Pattern):\n",
        "     - The spread of residuals increases or decreases as the fitted values increase.\n",
        "     - Appears like a funnel or cone, indicating increasing or decreasing variance.\n",
        "\n",
        "  - Systematic Patterns:\n",
        "    - Any non-random pattern (e.g., curves, clusters) in the residual plot suggests that the variance is not constant.\n",
        "\n",
        "  - Spread Varies Across Values:\n",
        "    - Residuals are tightly clustered in one region and widely spread in another.\n",
        "\n",
        "- Example:A scatterplot of residuals that starts narrow and gradually fans out as the predicted values increase is a classic sign of heteroscedasticity.\n",
        "\n",
        "- Why It Is Important to Address Heteroscedasticity >>\n",
        " - Biased Standard Errors:\n",
        "   - Heteroscedasticity leads to incorrect standard error estimates, which affects hypothesis tests and confidence intervals.\n",
        "   - As a result, t-tests and p-values may become unreliable, leading to incorrect conclusions about the significance of coefficients.\n",
        "\n",
        "  - Inefficient Estimators:\n",
        "    - While OLS estimators remain unbiased, they are no longer efficient (i.e., they do not have the smallest possible variance).\n",
        "    - This weakens the predictive power of the model.\n",
        "\n",
        "  - Violates Regression Assumptions:\n",
        "    - It violates one of the core OLS assumptions, compromising the validity of statistical inference.\n",
        "\n",
        "  - Affects Model Interpretation:\n",
        "    - When variance of errors is not constant, the impact of predictor variables on the dependent variable might appear stronger or weaker than it actually is.\n",
        "\n",
        "- Addressing Heteroscedasticity:To deal with heteroscedasticity, the following methods can be used:\n",
        " - Data Transformation: Apply transformations like log, square root, or Box-Cox to stabilize variance.\n",
        " - Weighted Least Squares (WLS): Assign weights to observations to reduce the effect of non-constant variance.\n",
        " - Robust Standard Errors: Use heteroscedasticity-consistent standard errors to make inference valid.\n",
        " - Redefine Model: Revisit variable selection and check for omitted variables that could explain the variance."
      ],
      "metadata": {
        "id": "_dOg5Q87Cz73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "- In multiple linear regression, both R² and Adjusted R² are used to assess the goodness-of-fit of the model. However, they serve slightly different purposes:\n",
        " - R² (Coefficient of Determination): Measures the proportion of variance in the dependent variable that is explained by the independent variables.\n",
        " - djusted R²: Adjusts the R² value for the number of predictors in the model. It penalizes the model for including unnecessary or irrelevant variables.\n",
        "\n",
        "- Interpretation of High R² but Low Adjusted R²: If a multiple regression model shows a high R² but a low adjusted R², it generally indicates that:\n",
        "\n",
        "- Unnecessary Variables Are Included:\n",
        "  - The model contains one or more independent variables that do not contribute meaningfully to explaining the variability in the dependent variable.\n",
        "  - These variables inflate the R² value without improving the model’s actual performance.\n",
        "\n",
        "- Overfitting the Model:\n",
        "  - The model may be overfitting the training data by including too many predictors, especially ones that are not statistically significant.\n",
        "  - This leads to a model that looks good on paper but performs poorly on unseen data.\n",
        "\n",
        "- Poor Model Quality Despite High R²:\n",
        "  - A high R² alone may give a false sense of model accuracy, but the low adjusted R² corrects for this illusion, signaling that the model is not as strong as it appears.\n",
        "\n",
        "- Adjusted R² Can Decrease with More Predictors:\n",
        "  - If added variables do not improve the model significantly, adjusted R² can decrease, while R² can still increase or remain high.\n",
        "\n",
        "- Example:Suppose a model predicting house prices includes:\n",
        " - Meaningful variables: area, location, number of rooms.\n",
        " - Irrelevant variables: house color, name of street, owner's birth year.\n",
        "\n",
        "  - Including the irrelevant variables may result in a high R² due to increased complexity, but the adjusted R² will be lower, indicating the model is not genuinely improved.\n",
        "\n",
        "\n",
        "- A high R² but low adjusted R² suggests that the model includes predictors that do not significantly contribute to explaining the target variable and may lead to overfitting. It serves as a warning to simplify the model by removing redundant or non-informative variables, thereby improving both the interpretability and the predictive power of the regression model."
      ],
      "metadata": {
        "id": "G2T9rwD3C0Ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "- In multiple linear regression, scaling variables refers to transforming features so that they are on a similar scale, typically through standardization (z-score) or normalization (min-max scaling). Although scaling is not always strictly necessary for regression to function, it becomes important in several scenarios, especially when the model includes variables with different units or magnitudes.\n",
        "\n",
        "- Reasons Why Scaling is Important:\n",
        " - Improves Interpretability of Coefficients:\n",
        "   - When variables are on the same scale, it is easier to compare the relative importance of each predictor.\n",
        "   - Coefficients become more meaningful because they are based on comparable units.\n",
        "\n",
        " - Enhances Numerical Stability:\n",
        "   - Variables with widely different magnitudes can lead to numerical instability in the matrix calculations (e.g., inversion of XᵀX).\n",
        "   - Scaling reduces the risk of computational errors, especially in models with a large number of variables.\n",
        "\n",
        " - Important for Regularized Regression:\n",
        "   - In Ridge and Lasso Regression, unscaled variables can bias the regularization penalty, as coefficients for larger-scaled features may be penalized more heavily.\n",
        "   - Scaling ensures that each feature is penalized equally, which is critical for fair and accurate model training.\n",
        "\n",
        " - Helps in Feature Selection and Gradient Descent (if used):\n",
        "   - If optimization techniques like gradient descent are used to estimate coefficients, scaling speeds up convergence.\n",
        "   - Unscaled data can slow down or even mislead the optimization process due to uneven step sizes.\n",
        "\n",
        "\n",
        "- Scaling variables in multiple linear regression is essential when features differ significantly in their scales or units. It ensures numerical stability, enhances model interpretability, and is critical when using regularization techniques. Therefore, scaling is considered a best practice, especially in models involving complex or real-world datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "7DtUm6G4C0NE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression ?\n",
        "- Polynomial Regression is a type of regression analysis where the relationship between the independent variable (x) and the dependent variable (y) is modeled as an nth-degree polynomial. It is an extension of linear regression, used when the data shows a non-linear relationship that a straight line cannot capture accurately.\n",
        "- Mathematical Form:\n",
        "   - The general form of a polynomial regression model is:\n",
        "          y = β0 + β1*x + β2*x^2 + β3*x^3 + ... + βn*x^n + ε\n",
        "          Where:\n",
        "          y = predicted value (dependent variable)\n",
        "          x = independent variable\n",
        "          β0, β1, ..., βn = regression coefficients\n",
        "          ε = error term\n",
        "          n = degree of the polynomial\n",
        "\n",
        "- Why Polynomial Regression is Used:\n",
        " - Captures Non-linear Relationships:\n",
        "When data shows curves or bends, polynomial regression can model that pattern better than linear regression.\n",
        "- Better Fit for Complex Data:\n",
        "For data that increases or decreases at a non-constant rate, polynomial terms allow the model to follow the curve more closely.\n",
        "\n",
        "- Polynomial regression is a powerful technique for modeling non-linear relationships using a linear model framework with transformed features. It allows the regression curve to bend and twist to fit the data, but should be used with care to avoid overfitting.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g_4nnrfeC0VZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does Polynomial Regression differ from Linear Regression ?\n",
        "- Linear Regression and Polynomial Regression are both techniques used to model relationships between a dependent variable and one or more independent variables. However, they differ in terms of the model’s complexity and the type of relationships they can represent\n",
        "\n",
        "- 1. Model Form >>\n",
        "  - Polynomial Regression:\n",
        "    - The relationship between the dependent and independent variable is modeled as a polynomial of degree n.\n",
        "    - The model is expressed as:\n",
        "                y = β0 + β1*x + β2*x^2 + β3*x^3 + ... + βn*x^n + ε\n",
        "\n",
        "                where\n",
        "                y = dependent variable\n",
        "                x = independent variable\n",
        "                β0, β1 = coefficients\n",
        "                ε = error term\n",
        "                n = degree of the polynomial (e.g., 2 for quadratic, 3 for cubic)\n",
        "  - Linear Regression:\n",
        "    - The relationship between the dependent and independent variable is assumed to be linear.\n",
        "    - The model is expressed as a straight line in the form:\n",
        "                y = β0 + β1*x + ε\n",
        "                where\n",
        "                y = dependent variable\n",
        "                x = independent variable\n",
        "                β0, β1 = coefficients\n",
        "                ε = error term\n",
        "\n",
        "- 2. Relationship Type >>\n",
        "  - Polynomial Regression:\n",
        "    -  Captures non-linear relationships, allowing for curves and bends in the data.\n",
        "   - Best suited for data where the relationship between variables is not straight and needs higher-order terms to fit the curve (e.g., growth rate, acceleration).\n",
        "      \n",
        " - Linear Regression:\n",
        "   - Assumes a straight-line relationship between the dependent and independent variables.\n",
        "   - Best suited for data where the relationship can be captured by a linear function (e.g., price vs. quantity sold).\n",
        "\n",
        "- 3. Flexibility >>\n",
        " - Polynomial Regression:\n",
        "   - More flexible, as it can fit curves of varying degrees (quadratic, cubic, etc.).\n",
        "   - Can fit data that shows a curved pattern, making it suitable for more complex, non-linear data.\n",
        "\n",
        " - Linear Regression:\n",
        "   - Less flexible, as it only fits straight lines. It is limited to simple linear trends.\n",
        "   - Doesn’t capture complex patterns well in data that is inherently non-linear.\n",
        "\n",
        "- 4. Overfitting Risk >>\n",
        "  - Polynomial Regression:\n",
        "    - Higher-degree polynomial models are more prone to overfitting, especially when the degree is too high for the dataset.\n",
        "    - As the degree increases, the model may fit the training data too closely, capturing noise and irrelevant fluctuations, which can lead to poor generalization on new data.\n",
        "\n",
        "  - Linear Regression:\n",
        "    - Less prone to overfitting, especially when there are fewer predictors and when the relationship is genuinely linear.\n",
        "\n",
        "- 5. Interpretability >>\n",
        "  - Polynomial Regression:\n",
        "    - Harder to interpret as the relationship between the predictors and outcome is more complex.\n",
        "    - High-degree polynomial models may be difficult to explain clearly.\n",
        "\n",
        "  - Linear Regression:\n",
        "    - Easier to interpret, as the relationship between the variables is straightforward.\n",
        "    - Coefficients indicate the linear influence of each predictor on the outcome.\n",
        "\n",
        "- While Linear Regression is best suited for modeling linear relationships with simple data, Polynomial Regression is used when the data exhibits non-linear trends. Polynomial regression adds more complexity by introducing higher-order terms to the model, providing more flexibility but also increasing the risk of overfitting. Choosing between the two depends on the nature of the data and the underlying relationship.\n"
      ],
      "metadata": {
        "id": "WrzfD2DeC0eJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is Polynomial Regression used?\n",
        "- Polynomial Regression is used when the relationship between the independent variable and the dependent variable is non-linear, and a simple straight-line model (i.e., linear regression) is not able to capture the underlying pattern in the data. Instead of fitting a straight line, polynomial regression fits a curved line by including higher-degree powers of the independent variable.\n",
        "\n",
        "- Situations When Polynomial Regression is Used:\n",
        "\n",
        "- When the Data Shows a Curved Pattern:\n",
        "  - If a scatter plot of the data points suggests a non-linear trend, such as a parabolic or S-shaped curve, polynomial regression is often more appropriate.\n",
        "  - Example: Modeling the trajectory of a ball, where height depends on time in a curved path.\n",
        "\n",
        "- When Linear Regression Underfits:\n",
        "  - If a linear model results in high residuals or low performance, and the data appears to follow a non-linear pattern, using a polynomial regression model can improve accuracy.\n",
        "\n",
        "- In Engineering and Physics Problems:\n",
        "  - Many real-world phenomena such as acceleration, growth curves, and heat transfer follow polynomial relationships, especially when variables interact in non-linear ways.\n",
        "\n",
        "- In Business and Economics:\n",
        "  - It can be used to model diminishing returns, profit curves, or price-demand relationships, which often exhibit a non-linear trend.\n",
        "\n",
        "- When Higher Flexibility is Required:\n",
        "  - Polynomial regression allows for more flexibility than linear regression by adjusting the degree of the polynomial to fit complex relationships.\n",
        "\n",
        "\n",
        "- Polynomial regression is used when data shows a non-linear trend that cannot be effectively captured by a straight line. It is useful in fields like science, engineering, business, and economics where relationships between variables often follow a curved pattern. However, care should be taken to choose the right degree of the polynomial to avoid overfitting the model."
      ],
      "metadata": {
        "id": "QTbxHwEOC0mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for Polynomial Regression?\n",
        "- The general equation for Polynomial Regression represents the relationship between the dependent variable (y) and the independent variable (x) as a polynomial function of degree n. Unlike linear regression, which fits a straight line, polynomial regression fits a curved line by including higher-order terms of the predictor variable.\n",
        "\n",
        "       General Equation:\n",
        "       y = β0 + β1*x + β2*x^2 + β3*x^3 + ... + βn*x^n + ε\n",
        "       where:\n",
        "       y = dependent variable (predicted output)\n",
        "       x = independent variable (input feature)\n",
        "       β0 = intercept term\n",
        "       β1, β2, ..., βn = coefficients for each polynomial term\n",
        "       x^2, x^3, ..., x^n = higher-order terms (squared, cubic, etc.)\n",
        "       n = degree of the polynomial\n",
        "       ε = error term or residual\n",
        "\n",
        "\n",
        "- The general polynomial regression equation extends the simple linear regression model by adding non-linear terms to better fit complex data. The degree of the polynomial (n) controls how flexible the model is in capturing curved trends.\n",
        "\n"
      ],
      "metadata": {
        "id": "B2KmnW2dC0u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can Polynomial Regression be applied to multiple variables?\n",
        "- Yes, Polynomial Regression can be applied to multiple variables. When there is more than one independent variable, polynomial regression is extended to include polynomial terms and interaction terms for those variables. This is known as Multivariate Polynomial Regression.\n",
        "\n",
        "- Explanation:\n",
        "            In multivariate polynomial regression, the model includes:\n",
        "            Polynomial terms (e.g., x1^2, x2^3)\n",
        "            Interaction terms (e.g., x1x2, x1^2x2)\n",
        "\n",
        "      - These help to model more complex, non-linear relationships between the predictors and the target variable.\n",
        "\n",
        "- Polynomial regression can be effectively used with multiple variables by incorporating polynomial and interaction terms. This helps to capture complex patterns in the data. However, using too many variables or too high a degree can lead to overfitting, so it’s important to balance complexity with performance."
      ],
      "metadata": {
        "id": "tWxcBLKQC03n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of Polynomial Regression?\n",
        "- While Polynomial Regression is useful for modeling non-linear relationships, it also comes with several limitations that can affect its performance and interpretability. Understanding these limitations is important for choosing the right model for a given dataset.\n",
        "\n",
        "- Overfitting >>\n",
        "  - Polynomial regression can easily overfit the data, especially when the degree of the polynomial is too high.\n",
        "  - This means the model may fit the training data very well but perform poorly on new or unseen data.\n",
        "\n",
        "- Interpretability >>\n",
        "  - As the degree increases, the model becomes more complex and difficult to interpret.\n",
        "  - Higher-order terms and interactions between variables make it harder to understand the influence of each variable.\n",
        "\n",
        "- Sensitive to Outliers >>\n",
        "  - Polynomial regression is highly sensitive to extreme values.\n",
        "  - A few outliers can significantly distort the curve and lead to misleading results.\n",
        "\n",
        "- Poor Extrapolation >>\n",
        "  - Polynomial regression performs poorly when used for prediction outside the range of the training data.\n",
        "  - The curve can increase or decrease sharply beyond the data range, giving unrealistic predictions.\n",
        "\n",
        "- Increased Computational Complexity >>\n",
        "  - As the number of variables and the degree of the polynomial increase, the number of features grows rapidly.\n",
        "  - This can lead to longer training times and increased computational cost.\n",
        "\n",
        "- Risk of Multicollinearity >>\n",
        "  - Polynomial terms (like x, x², x³) are often highly correlated with each other.\n",
        "  - This multicollinearity can lead to unstable coefficient estimates and reduce model reliability.\n",
        "\n",
        "- Although polynomial regression is powerful for modeling non-linear trends, it must be used carefully. Overfitting, complexity, and sensitivity to outliers are common issues. It's important to choose the degree of the polynomial wisely and validate the model using cross-validation or regularization techniques when necessary.\n",
        "  "
      ],
      "metadata": {
        "id": "PIWfOfy9C1Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial ?\n",
        "- When using Polynomial Regression, selecting the appropriate degree of the polynomial is crucial to avoid underfitting or overfitting. Several methods can be used to evaluate the model fit and help decide the optimal polynomial degree.\n",
        "\n",
        "- R-squared (R²) and Adjusted R-squared:\n",
        "  - R-squared measures how well the model explains the variance in the target variable.\n",
        "  - Adjusted R-squared penalizes unnecessary complexity and is more reliable for comparing models with different numbers of predictors.\n",
        "  - If R² increases but adjusted R² decreases, it may indicate overfitting.\n",
        "\n",
        "- Cross-Validation (e.g., k-Fold Cross-Validation):\n",
        "  - This technique divides the data into training and validation sets multiple times.\n",
        "  - It helps assess how well the model generalizes to unseen data.\n",
        "  - By comparing cross-validation scores for different degrees, the best-fitting polynomial can be selected.\n",
        "\n",
        "- Mean Squared Error (MSE) / Root Mean Squared Error (RMSE):\n",
        " - MSE and RMSE calculate the average squared difference between predicted and actual values.\n",
        " - These are used to evaluate how well the model fits both training and validation data.\n",
        " - Lower values indicate better model performance.\n",
        "\n",
        "- Visual Inspection of the Fit:\n",
        "  - Plotting the data along with the fitted curve helps to visually assess the model.\n",
        "  - If the curve is too wavy or overly complex, it may suggest overfitting.\n",
        "  - A good fit should follow the trend of the data without being too erratic.\n",
        "\n",
        "- Learning Curves:\n",
        "  - A learning curve shows the model's performance on training and validation sets as the degree increases.\n",
        "  - If validation error starts increasing while training error decreases, it indicates overfitting.\n",
        "\n",
        "- To select the best degree for a polynomial regression model, it’s essential to use a combination of statistical metrics, validation techniques, and visual inspection. Cross-validation and adjusted R-squared are especially useful to balance model complexity and generalization ability."
      ],
      "metadata": {
        "id": "s7L2k51jC1JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in Polynomial Regression?\n",
        "- Visualization plays a crucial role in Polynomial Regression because it helps us understand how well the model captures the underlying pattern in the data, especially when the relationship between variables is non-linear and complex.\n",
        "\n",
        "- Identifying the Shape of the Relationship:\n",
        "  - Visualization allows us to observe whether the actual data points follow a curved pattern, which can justify the use of a polynomial model.\n",
        "  - It helps decide whether a linear, quadratic, cubic, or higher-degree polynomial is suitable.\n",
        "\n",
        "- Detecting Underfitting and Overfitting:\n",
        "  - A plot of the predicted curve along with actual data points makes it easier to spot if the model is:\n",
        "    - Underfitting (too simple and missing the curve)\n",
        "    - Overfitting (too complex and following every fluctuation)\n",
        "\n",
        "- Evaluating Model Fit:\n",
        "  - Visualization provides an intuitive way to check how closely the regression curve follows the data points.\n",
        "  - It complements numerical metrics like R² and RMSE by giving a clear picture of the model's behavior.\n",
        "\n",
        "- Enhancing Interpretability:\n",
        "  - Complex equations can be difficult to understand, especially with higher-degree terms.\n",
        "  - A visual graph simplifies interpretation, showing how the dependent variable changes with the independent variable(s).\n",
        "\n",
        "- Detecting Outliers and Irregular Patterns:\n",
        "  - Visualization helps highlight outliers, gaps, or clusters in the data that may affect model accuracy.\n",
        "  - This enables better data cleaning and preprocessing.\n",
        "\n",
        "- Communication and Reporting:\n",
        "  - Visual plots are helpful in presentations, reports, and dashboards.\n",
        "  - They make it easier to communicate insights to non-technical stakeholders.\n",
        "\n",
        "- Visualization is essential in polynomial regression for understanding the model's performance, detecting problems like overfitting, and communicating results clearly. It adds a visual layer of insight that complements statistical evaluation."
      ],
      "metadata": {
        "id": "Zj5VssUEC1S9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is Polynomial Regression implemented in Python?\n",
        "- Polynomial Regression in Python can be implemented using libraries like scikit-learn. The process involves converting the original features into polynomial features, fitting a linear regression model to those features, and then using the model to make predictions.\n",
        "\n",
        "- Import Required Libraries:\n",
        "           import numpy as np\n",
        "           import pandas as pd\n",
        "           import matplotlib.pyplot as plt\n",
        "           from sklearn.linear_model import LinearRegression\n",
        "           from sklearn.preprocessing import PolynomialFeatures\n",
        "           from sklearn.metrics import mean_squared_error\n",
        "\n",
        "- Prepare the Data:\n",
        "      # Example dataset\n",
        "      X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
        "      y = np.array([2.3, 2.9, 3.8, 4.4, 5.0, 6.8, 8.0, 9.5, 11.2, 12.8])\n",
        "\n",
        "- Create Polynomial Features:\n",
        "       poly = PolynomialFeatures(degree=2)  # Degree can be changed\n",
        "       X_poly = poly.fit_transform(X)\n",
        "\n",
        "- Fit the Model:\n",
        "       model = LinearRegression()\n",
        "       model.fit(X_poly, y)\n",
        "\n",
        "- Make Predictions:\n",
        "       y_pred = model.predict(X_poly)\n",
        "\n",
        "- Evaluate the Model:\n",
        "       mse = mean_squared_error(y, y_pred)\n",
        "       print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "- Visualize the Results:\n",
        "       plt.scatter(X, y, color='blue', label='Actual')\n",
        "       plt.plot(X, y_pred, color='red', label='Polynomial Fit')\n",
        "       plt.title('Polynomial Regression')\n",
        "       plt.xlabel('X')\n",
        "       plt.ylabel('y')\n",
        "       plt.legend()\n",
        "       plt.show()\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          "
      ],
      "metadata": {
        "id": "wKqddd2RC1dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#31 How is Polynomial Regression implemented in Python? (pratically solved)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Example dataset\n",
        "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
        "y = np.array([2.3, 2.9, 3.8, 4.4, 5.0, 6.8, 8.0, 9.5, 11.2, 12.8])\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)  # Degree can be changed\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "y_pred = model.predict(X_poly)\n",
        "\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Actual')\n",
        "plt.plot(X, y_pred, color='red', label='Polynomial Fit')\n",
        "plt.title('Polynomial Regression')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "uMwp3ZmhAjR3",
        "outputId": "501195e7-1786-419e-a413-729bf6b1f2c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.031216666666666688\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vQKOTEsmC1lx"
      }
    }
  ]
}